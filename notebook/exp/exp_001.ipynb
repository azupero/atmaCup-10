{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly import express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import texthero as hero\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import utils\n",
    "\n",
    "logger = utils.get_logger()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "INPUT_DIR = \"../../input\"\n",
    "OUTPUT_DIR = \"../../output\"\n",
    "EXP_NAME = \"exp_001\"\n",
    "\n",
    "OBJECT_ID = \"object_id\"\n",
    "\n",
    "wandb.init(project=\"atmacup-10\", name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtDataset(object):\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.target = 'likes'\n",
    "        self.file_path = file_path\n",
    "        self.train_df = pd.read_csv(self.file_path / 'train.csv')\n",
    "        self.test_df = pd.read_csv(self.file_path / 'test.csv')\n",
    "        self.color_df = pd.read_csv(self.file_path / 'color.csv')\n",
    "        self.historical_df = pd.read_csv(self.file_path / 'historical_person.csv')\n",
    "        self.maker_df = pd.read_csv(self.file_path / 'maker.csv')\n",
    "        self.material_df = pd.read_csv(self.file_path / 'material.csv')\n",
    "        self.object_df = pd.read_csv(self.file_path / 'object_collection.csv')\n",
    "        self.palette_df = pd.read_csv(self.file_path / 'palette.csv')\n",
    "        self.principal_occupation_df = pd.read_csv(self.file_path / 'principal_maker_occupation.csv')\n",
    "        self.principal_maker_df =  pd.read_csv(self.file_path / 'principal_maker.csv')\n",
    "        self.production_df = pd.read_csv(self.file_path / 'production_place.csv')\n",
    "        self.technique_df = pd.read_csv(self.file_path / 'technique.csv')\n",
    "        self.submission = pd.read_csv(self.file_path / 'sample_submission.csv')\n",
    "\n",
    "    def get_whole_df(self):\n",
    "        return pd.concat([self.train_df, self.test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    def get_target(self, log: bool = False):\n",
    "        return np.log1p(self.train_df[self.target].values) if log else self.train_df[self.target].values\n",
    "\n",
    "art_ds = ArtDataset(file_path=Path(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBlock(object):\n",
    "    def fit(self, input_df: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return NotImplementedError()\n",
    "\n",
    "\n",
    "class OrdinalEncodingBlock(BaseBlock):\n",
    "    def __init__(self, cat_cols: list):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.encoder = ce.OrdinalEncoder(handle_unknown=\"value\", handle_missing=\"value\")\n",
    "        self.encoder.fit(input_df[self.cat_cols])\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return (\n",
    "            self.encoder.transform(input_df[self.cat_cols])\n",
    "            .add_prefix(\"OE_\")\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "\n",
    "class CountEncodingBlock(BaseBlock):\n",
    "    def __init__(self, cat_cols: list):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.encoder = ce.CountEncoder(handle_unknown=-1, handle_missing=\"count\")\n",
    "        self.encoder.fit(input_df[self.cat_cols])\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return self.encoder.transform(input_df[self.cat_cols]).add_prefix(\"CE_\")\n",
    "\n",
    "\n",
    "class StringLengthBlock(BaseBlock):\n",
    "    '''文字列の長さを返す\n",
    "    '''\n",
    "    def __init__(self, columns: list):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = pd.DataFrame()\n",
    "        for col in self.columns:\n",
    "            output_df[col] = input_df[col].str.len()\n",
    "\n",
    "        return output_df.add_prefix(\"StringLength_\")\n",
    "\n",
    "\n",
    "def merge_by_key(left: Union[pd.DataFrame, pd.Series], right: pd.DataFrame, on=OBJECT_ID) -> pd.DataFrame:\n",
    "    if not isinstance(left, pd.Series):\n",
    "        left = left[on]\n",
    "    return pd.merge(left, right, on=on, how='left').drop(columns=[on])\n",
    "    \n",
    "\n",
    "def text_normalization(text):\n",
    "    '''textheroを用いたテキスト前処理pipeline\n",
    "    '''\n",
    "    # nltkのオランダ語と英語のstopword\n",
    "    custom_stopwords = nltk.corpus.stopwords.words('dutch') + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    clean_text = hero.clean(\n",
    "        text, \n",
    "        pipeline=[\n",
    "            hero.preprocessing.fillna,\n",
    "            hero.preprocessing.lowercase,\n",
    "            hero.preprocessing.remove_digits,\n",
    "            hero.preprocessing.remove_punctuation,\n",
    "            hero.preprocessing.remove_diacritics,\n",
    "            lambda x: hero.preprocessing.remove_stopwords(x, stopwords=custom_stopwords),\n",
    "            hero.preprocessing.remove_whitespace,\n",
    "        ])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "class TfidfBlock(BaseBlock):\n",
    "    '''TF-IDF特徴量\n",
    "    - 複数カラムに対応\n",
    "    '''\n",
    "    def __init__(self, column: str, n_components: int = 50):\n",
    "        self.column = column\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def get_text_series(self, input_df: pd.DataFrame) -> pd.Series:\n",
    "        '''input_dfを入力としてテキスト正規化したpd.Seriesを返す\n",
    "        &で連結された複数カラムの場合は空白区切りでテキストを連結させる\n",
    "        '''\n",
    "        out_series = None\n",
    "\n",
    "        for i, col in enumerate(self.column.split('&')):\n",
    "            text_i = text_normalization(input_df[col]).astype(str)\n",
    "            if out_series is None:\n",
    "                out_series = text_i\n",
    "            else:\n",
    "                out_series = out_series + ' ' + text_i\n",
    "\n",
    "        return out_series\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame):\n",
    "        whole_df = art_ds.get_whole_df()\n",
    "        x = self.get_text_series(whole_df)\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000)),\n",
    "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=42)),\n",
    "        ])\n",
    "\n",
    "        feature = self.pipeline.fit_transform(x)\n",
    "        self.agg_df = pd.concat([whole_df[[OBJECT_ID]].copy(), pd.DataFrame(feature)], axis=1)\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return merge_by_key(input_df, self.agg_df).add_prefix(f\"{self.column}_Tfidf_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_input_df():\n",
    "    '''w2vのinputのdataframeを作成する\n",
    "    - object_idをPKとするdataframeを返すことを想定\n",
    "    '''\n",
    "    # whole_df(train/test)\n",
    "    whole_df = art_ds.get_whole_df()\n",
    "    whole_df[\"title\"] = whole_df[\"title\"].astype(str)\n",
    "    whole_df[\"description\"] = whole_df[\"description\"].astype(str)\n",
    "    whole_df[\"more_title\"] = whole_df[\"more_title\"].astype(str)\n",
    "    whole_df[\"title-description\"] = whole_df[\"title\"].astype(str) + ' ' + whole_df[\"description\"].astype(str)\n",
    "\n",
    "    w2v_whole_title = whole_df.groupby([\"object_id\"])[\"title\"].apply(list).reset_index()\n",
    "    w2v_whole_description = whole_df.groupby([\"object_id\"])[\"description\"].apply(list).reset_index()\n",
    "    w2v_whole_more_title = whole_df.groupby([\"object_id\"])[\"more_title\"].apply(list).reset_index()\n",
    "    w2v_whole_title_description = whole_df.groupby([\"object_id\"])[\"title-description\"].apply(list).reset_index()\n",
    "    \n",
    "    # material\n",
    "    material_df = art_ds.material_df.copy()\n",
    "    w2v_material = material_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # object\n",
    "    object_df = art_ds.object_df.copy()\n",
    "    w2v_object = object_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # technique\n",
    "    technique_df = art_ds.technique_df.copy()\n",
    "    w2v_technique = technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/object\n",
    "    material_object_df = pd.concat([material_df, object_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_object = material_object_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/technique\n",
    "    material_technique_df = pd.concat([material_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_technique = material_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # object/technique\n",
    "    object_technique_df = pd.concat([object_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_object_technique = object_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/object/technique\n",
    "    material_object_technique_df = pd.concat([material_df, object_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_object_technique = material_object_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    returns = {\n",
    "        \"w2v_whole_title\": w2v_whole_title,\n",
    "        \"w2v_whole_description\": w2v_whole_description,\n",
    "        \"w2v_whole_more_title\": w2v_whole_more_title,\n",
    "        \"w2v_whole_title_description\": w2v_whole_title_description,\n",
    "        \"w2v_material\": w2v_material,\n",
    "        \"w2v_object\": w2v_object,\n",
    "        \"w2v_technique\": w2v_technique,\n",
    "        \"w2v_material_object\": w2v_material_object,\n",
    "        \"w2v_material_technique\": w2v_material_technique,\n",
    "        \"w2v_object_technique\": w2v_object_technique,\n",
    "        \"w2v_material_object_technique\": w2v_material_object_technique,\n",
    "    }\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "class W2vBlock(BaseBlock):\n",
    "    '''Word2Vecによる単語ベクトル表現を得て、平均により文章ベクトル化\n",
    "    - 単一dataframeで複数カラムに対応\n",
    "    '''\n",
    "    def __init__(self, sentences_df: pd.DataFrame, df_name: str, size: int, min_count: int, window: int, num_iter: int):\n",
    "        self.sentences_df = sentences_df\n",
    "        self.df_name = df_name\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.agg_df = art_ds.get_whole_df()[['object_id']]\n",
    "        cat_col = self.sentences_df.columns.drop(\"object_id\")[0]\n",
    "        # text normalization\n",
    "        self.sentences_df[cat_col] = text_normalization(self.sentences_df[cat_col])\n",
    "\n",
    "        self.w2v_model = word2vec.Word2Vec(\n",
    "            self.sentences_df[cat_col].values.tolist(),\n",
    "            size=self.size,\n",
    "            min_count=self.min_count,\n",
    "            window=self.window,\n",
    "            iter=self.num_iter,\n",
    "        )\n",
    "\n",
    "        # element-wise average(SWEM-aver)\n",
    "        sentence_vectors = self.sentences_df[cat_col].progress_apply(lambda x: np.mean([self.w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack(sentence_vectors)\n",
    "        sentence_vectors_df = pd.DataFrame(sentence_vectors, columns=[f\"{self.df_name}_{i}\" for i in range(self.size)])\n",
    "        sentence_vectors_df.index = self.sentences_df[\"object_id\"]\n",
    "        self.agg_df = self.agg_df.merge(sentence_vectors_df, on='object_id', how='left')\n",
    "        \n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return merge_by_key(input_df, self.agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(train_df: pd.DataFrame, test_df: pd.DataFrame, y, blocks: list) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_feat_df = pd.DataFrame()\n",
    "    test_feat_df = pd.DataFrame()\n",
    "\n",
    "    # train_df = preprocess(train_df)\n",
    "    # test_df = preprocess(test_df)\n",
    "\n",
    "    for block in blocks:\n",
    "        with utils.timer(name=f\"{str(block) + '_fit'}\", logger=logger):\n",
    "            try:\n",
    "                out_train_block = block.fit(train_df, y=y)\n",
    "            except Exception as e:\n",
    "                print(f'Error on {block} fit. ')\n",
    "                raise e from e\n",
    "\n",
    "        train_feat_df = pd.concat([train_feat_df, out_train_block], axis=1)\n",
    "\n",
    "    for block in blocks:\n",
    "        with utils.timer(name=f\"{str(block) + '_transform'}\", logger=logger):\n",
    "            out_test_block = block.transform(test_df)\n",
    "\n",
    "        test_feat_df = pd.concat([test_feat_df, out_test_block], axis=1)\n",
    "\n",
    "    return train_feat_df, test_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_k_fold(train_df: pd.DataFrame, group_col: str, n_splits: int, seed: int) -> pd.DataFrame:\n",
    "    '''GroupKFoldで分割してfold列を付与する\n",
    "    '''\n",
    "    _train_df = train_df.copy()\n",
    "    group_series = _train_df[group_col]\n",
    "    group_key = group_series.unique()\n",
    "\n",
    "    splitter = KFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    for fold_id, (_, val_group_idx) in enumerate(splitter.split(group_key)):\n",
    "        val_group = group_key[val_group_idx]\n",
    "        is_val = group_series.isin(val_group)\n",
    "\n",
    "        _train_df.loc[is_val, \"fold\"] = fold_id\n",
    "\n",
    "    _train_df[\"fold\"] = _train_df[\"fold\"].astype(int)\n",
    "\n",
    "    return _train_df[\"fold\"].values\n",
    "\n",
    "\n",
    "def get_stratified_k_fold(train_df: pd.DataFrame, y: pd.Series, n_splits: int, seed: int) -> pd.DataFrame:\n",
    "    '''StratifiedK-Foldで分割してfold列を付与\n",
    "    '''\n",
    "    _train_df = train_df.copy()\n",
    "    X = train_df.drop(columns=['likes'], axis=1)\n",
    "    y = y.round()\n",
    "\n",
    "    splitter = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    for fold_id, (_, valid_idx) in enumerate(splitter.split(X, y)):\n",
    "        _train_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    _train_df[\"fold\"] = _train_df[\"fold\"].astype(int)\n",
    "\n",
    "    return _train_df[\"fold\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        params: dict,\n",
    "        fold: np.ndarray,\n",
    "        seeds: list,\n",
    "    ):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.params = params\n",
    "        self.fold = fold\n",
    "        self.seeds = seeds\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self):\n",
    "        oof = np.zeros((len(self.seeds), len(self.y_train)))\n",
    "        num_fold = np.unique(self.fold).size\n",
    "\n",
    "        for i, seed in enumerate(self.seeds):\n",
    "            oof_ = np.zeros((len(self.y_train)))\n",
    "            self.params[\"seed\"] = seed\n",
    "\n",
    "            for fold_id in range(num_fold):\n",
    "                tr_idx = np.where(self.fold != fold_id)\n",
    "                val_idx = np.where(self.fold == fold_id)\n",
    "\n",
    "                X_train_fold = self.X_train[tr_idx]\n",
    "                X_valid_fold = self.X_train[val_idx]\n",
    "\n",
    "                y_train_fold = self.y_train[tr_idx]\n",
    "                y_valid_fold = self.y_train[val_idx]\n",
    "\n",
    "                train_set = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "                valid_set = lgb.Dataset(X_valid_fold, y_valid_fold, reference=train_set)\n",
    "\n",
    "                model = lgb.train(\n",
    "                    train_set=train_set,\n",
    "                    valid_sets=[train_set, valid_set],\n",
    "                    params=self.params,\n",
    "                    verbose_eval=100,\n",
    "                    callbacks=[wandb_callback()]\n",
    "                )\n",
    "\n",
    "                oof_[val_idx] = model.predict(\n",
    "                    X_valid_fold, num_iteration=model.best_iteration\n",
    "                )\n",
    "                self.models.append(model)\n",
    "\n",
    "            oof[i, :] = oof_\n",
    "        y_oof = np.mean(oof, axis=0)\n",
    "\n",
    "        return y_oof, self.models\n",
    "\n",
    "    def predict(self):\n",
    "        y_pred = np.mean(\n",
    "            [model.predict(self.X_test) for model in self.models], axis=0\n",
    "        )\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 5,\n",
    "    \"num_leaves\": 32,\n",
    "    \"lambda_l1\": 0.01,\n",
    "    \"lambda_l2\": 0.01,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"num_threads\": 8,\n",
    "    \"verbosity\": -1,\n",
    "    \"num_iterations\": 10000,\n",
    "    \"early_stopping_round\": 100,\n",
    "}\n",
    "wandb.config.update(lgb_params)\n",
    "\n",
    "train_df = art_ds.train_df\n",
    "test_df = art_ds.test_df\n",
    "y_train = art_ds.get_target(log=True)\n",
    "fold = get_stratified_k_fold(train_df, y_train, 5, 42)\n",
    "\n",
    "blocks = [\n",
    "    OrdinalEncodingBlock(\n",
    "        cat_cols=[\n",
    "            \"principal_maker\",\n",
    "            \"principal_or_first_maker\",\n",
    "            \"copyright_holder\",\n",
    "            \"acquisition_method\",\n",
    "        ]\n",
    "    ),\n",
    "    CountEncodingBlock(\n",
    "        cat_cols=[\n",
    "            \"principal_maker\",\n",
    "            \"principal_or_first_maker\",\n",
    "            \"acquisition_method\",\n",
    "        ]\n",
    "    ),\n",
    "    StringLengthBlock(\n",
    "        columns=[\n",
    "            'title',\n",
    "            'description',\n",
    "            'long_title',\n",
    "            'more_title',\n",
    "            'sub_title',\n",
    "        ]\n",
    "    ),\n",
    "    # *[W2vBlock(\n",
    "    #     sentences_df=sentences_df,\n",
    "    #     df_name=df_name,\n",
    "    #     size=50,\n",
    "    #     min_count=1,\n",
    "    #     window=5,\n",
    "    #     num_iter=100\n",
    "    #     )\n",
    "    #     for df_name, sentences_df in get_w2v_input_df().items()],\n",
    "]\n",
    "\n",
    "X_train, X_test = create_feature(train_df, test_df, art_ds.get_target(), blocks)\n",
    "\n",
    "trainer = LightGBMTrainer(X_train.values, y_train, X_test.values, lgb_params, fold, [42, 2021, 2434, 1123, 98])\n",
    "y_oof, models = trainer.fit()\n",
    "y_pred = trainer.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importance(models, feat_train_df) -> plotly.graph_objects.Figure:\n",
    "    '''LightGBMのfeature importanceを可視化\n",
    "    '''\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importance(importance_type=\"gain\")\n",
    "        _df['feature'] = feat_train_df.columns\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], \n",
    "                                          axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby('feature')\\\n",
    "        .sum()[['feature_importance']]\\\n",
    "        .sort_values('feature_importance', ascending=False).index[:50]\n",
    "    \n",
    "    fig = px.box(\n",
    "        feature_importance_df.query(\"feature in @order\"),\n",
    "        x=\"feature_importance\",\n",
    "        y=\"feature\",\n",
    "        category_orders={\"feature\": order},\n",
    "        width=1250,\n",
    "        height=900,\n",
    "        title=\"Top 50 feature importance\",\n",
    "    )\n",
    "    fig.update_yaxes(showgrid=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_oof_predict_distribution(y_train: np.ndarray, y_oof: np.ndarray, y_pred: np.ndarray) -> plotly.graph_objects.Figure:\n",
    "    '''train, oof, predのhistogramを可視化する\n",
    "    '''\n",
    "    dfs = [pd.DataFrame({'phase': phase, 'value': value}) for phase, value in zip(['train', 'oof', 'pred'], [y_train, y_oof, y_pred])]\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    fig = px.histogram(df, x=\"value\", color=\"phase\", width=1250, marginal=\"box\")\n",
    "    fig.update_layout(barmode=\"overlay\")\n",
    "    fig.update_traces(opacity=0.6, marker=dict(line=dict(width=2,color=\"DarkSlateGrey\")))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_feature_importance(models, X_train)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_oof_predict_distribution(y_train, y_oof, y_pred)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_log_target(y_log: np.ndarray) -> np.ndarray:\n",
    "    y = np.expm1(y_log)\n",
    "    y = np.where(y < 0, 0, y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_submit(submission: pd.DataFrame, y_pred: np.ndarray, path: str, f_name: str):\n",
    "    dir_path = Path(path)\n",
    "    submission_df = submission.copy()\n",
    "    submission_df[\"likes\"] = y_pred\n",
    "    assert len(submission_df) == len(y_pred)\n",
    "\n",
    "    submission_df.to_csv(dir_path / str(f_name + \".csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_pred = revert_log_target(y_pred)\n",
    "make_submit(art_ds.submission, revert_pred, OUTPUT_DIR, EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}