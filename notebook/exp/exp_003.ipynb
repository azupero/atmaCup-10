{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp 003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly import express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import texthero as hero\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, List\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import utils\n",
    "\n",
    "# logger = utils.get_logger()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "INPUT_DIR = \"../../input\"\n",
    "OUTPUT_DIR = \"../../output\"\n",
    "EXP_NAME = \"exp_003\"\n",
    "\n",
    "OBJECT_ID = \"object_id\"\n",
    "\n",
    "# wandb.init(project=\"atmacup-10\", name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtDataset(object):\n",
    "    def __init__(self, file_path: Path):\n",
    "        self.target = 'likes'\n",
    "        self.file_path = file_path\n",
    "        self.train_df = pd.read_csv(self.file_path / 'train.csv')\n",
    "        self.test_df = pd.read_csv(self.file_path / 'test.csv')\n",
    "        self.color_df = pd.read_csv(self.file_path / 'color.csv')\n",
    "        self.historical_df = pd.read_csv(self.file_path / 'historical_person.csv')\n",
    "        self.maker_df = pd.read_csv(self.file_path / 'maker.csv')\n",
    "        self.material_df = pd.read_csv(self.file_path / 'material.csv')\n",
    "        self.object_df = pd.read_csv(self.file_path / 'object_collection.csv')\n",
    "        self.palette_df = pd.read_csv(self.file_path / 'palette.csv')\n",
    "        self.principal_occupation_df = pd.read_csv(self.file_path / 'principal_maker_occupation.csv')\n",
    "        self.principal_maker_df =  pd.read_csv(self.file_path / 'principal_maker.csv')\n",
    "        self.production_df = pd.read_csv(self.file_path / 'production_place.csv')\n",
    "        self.technique_df = pd.read_csv(self.file_path / 'technique.csv')\n",
    "        self.submission = pd.read_csv(self.file_path / 'sample_submission.csv')\n",
    "\n",
    "    def get_whole_df(self):\n",
    "        return pd.concat([self.train_df, self.test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    def get_target(self, log: bool = False):\n",
    "        return np.log1p(self.train_df[self.target].values) if log else self.train_df[self.target].values\n",
    "\n",
    "art_ds = ArtDataset(file_path=Path(INPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table / aggregation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBlock(object):\n",
    "    def fit(self, input_df: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return NotImplementedError()\n",
    "\n",
    "\n",
    "class OrdinalEncodingBlock(BaseBlock):\n",
    "    def __init__(self, cat_cols: list):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.encoder = ce.OrdinalEncoder(handle_unknown=\"value\", handle_missing=\"value\")\n",
    "        self.encoder.fit(input_df[self.cat_cols])\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return (\n",
    "            self.encoder.transform(input_df[self.cat_cols])\n",
    "            .add_prefix(\"OE_\")\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "\n",
    "class CountEncodingBlock(BaseBlock):\n",
    "    def __init__(self, cat_cols: list):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.encoder = ce.CountEncoder(handle_unknown=-1, handle_missing=\"count\")\n",
    "        self.encoder.fit(input_df[self.cat_cols])\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return self.encoder.transform(input_df[self.cat_cols]).add_prefix(\"CE_\")\n",
    "\n",
    "\n",
    "class TargetEncodingBlock(BaseBlock):\n",
    "    '''TargetEncoding\n",
    "    - mapping_dfはtest setに対する変換のため(train全体でTE)\n",
    "    - xfeatのTargetEncodingはoofにしかない水準の時の値が0になる仕様なのでoof平均で埋めるようにしている\n",
    "    - smoothingも実装したい\n",
    "    '''\n",
    "    def __init__(self, columns: List[str], target_column: str, cv: List[np.ndarray]):\n",
    "        self.columns = columns\n",
    "        self.target_column = target_column\n",
    "        self.cv = cv\n",
    "        self.num_fold = len(self.cv)\n",
    "\n",
    "    def create_mapping(self, input_df: pd.DataFrame, y: np.ndarray):\n",
    "        self.mapping_df = {}\n",
    "        self.y_mean = np.mean(y)\n",
    "        output_df = pd.DataFrame()\n",
    "\n",
    "        for col in self.columns:\n",
    "            keys = input_df[col].unique()\n",
    "            oof = np.zeros_like(input_df[col], dtype=float)\n",
    "\n",
    "            for train_idx, valid_idx in self.cv:\n",
    "                _df = input_df.iloc[train_idx].groupby([col])[self.target_column].mean()\n",
    "                _df = _df.reindex(keys)\n",
    "                _df = _df.fillna(_df.mean())\n",
    "                oof[valid_idx] = input_df.iloc[valid_idx][col].map(_df.to_dict())\n",
    "\n",
    "            output_df[col] = oof\n",
    "\n",
    "            self.mapping_df[col] = input_df.groupby([col])[self.target_column].mean()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        output_df = self.create_mapping(input_df, y)\n",
    "\n",
    "        return output_df.add_prefix(\"TE_\")\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = pd.DataFrame()\n",
    "\n",
    "        for col in self.columns:\n",
    "            output_df[col] = input_df[col].map(self.mapping_df[col]).fillna(self.y_mean)\n",
    "\n",
    "        return output_df.add_prefix(\"TE_\")\n",
    "\n",
    "\n",
    "def merge_by_key(left: Union[pd.DataFrame, pd.Series], right: pd.DataFrame, on=OBJECT_ID) -> pd.DataFrame:\n",
    "    if not isinstance(left, pd.Series):\n",
    "        left = left[on]\n",
    "    return pd.merge(left, right, on=on, how='left').drop(columns=[on])\n",
    "\n",
    "\n",
    "class OtherTableCountBlock(BaseBlock):\n",
    "    '''material, technique, production_place, historical_person, object_collectionのcrosstab集計\n",
    "    - nameが被集計対象\n",
    "    '''\n",
    "    def __init__(self, category_df: pd.DataFrame, df_name: str, minimum_freq: int) -> None:\n",
    "        self.category_df = category_df\n",
    "        self.df_name = df_name\n",
    "        self.minimum_freq = minimum_freq\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        vc = self.category_df[\"name\"].value_counts()\n",
    "        use_names = vc[vc >= self.minimum_freq].index\n",
    "        _df = self.category_df[self.category_df[\"name\"].isin(use_names)].reset_index(drop=True)\n",
    "        self.agg_df = pd.crosstab(_df[\"object_id\"], _df[\"name\"]).reset_index()\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = merge_by_key(input_df, self.agg_df).fillna(0).astype(int)\n",
    "        return output_df.add_prefix(f\"OtherTableCount_{self.df_name}_\")\n",
    "\n",
    "\n",
    "def detect_size_from_subtitle(input_df: pd.DataFrame):\n",
    "    output_df = pd.DataFrame()\n",
    "    for axis in ['h', 'w', 't', 'd']:\n",
    "        column_name = f'size_{axis}'\n",
    "        size_info = input_df['sub_title'].str.extract(r'{} (\\d*|\\d*\\.\\d*)(cm|mm)'.format(axis)) # 正規表現を使ってサイズを抽出\n",
    "        size_info = size_info.rename(columns={0: column_name, 1: 'unit'})\n",
    "        size_info[column_name] = size_info[column_name].replace('', np.nan).astype(float) # dtypeがobjectになってるのでfloatに直す\n",
    "        size_info[column_name] = size_info.apply(lambda row: row[column_name] * 10 if row['unit'] == 'cm' else row[column_name], axis=1) # 単位をmmに統一する\n",
    "        output_df[column_name] = size_info[column_name]\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "class SubtitleSizeBlock(BaseBlock):\n",
    "    '''sub_titleから作品のサイズ(W,H,t,D)/面積/アスペクト比/欠損している属性数を抽出する\n",
    "    '''\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = detect_size_from_subtitle(input_df)\n",
    "        output_df[\"area\"] = output_df[\"size_w\"] * output_df[\"size_h\"]\n",
    "        output_df[\"aspect\"] = output_df[\"size_w\"] / output_df[\"size_h\"]\n",
    "        output_df[\"missing_attributes\"] = output_df.isnull().sum(axis=1)\n",
    "\n",
    "        return output_df.add_prefix(\"Subtitle_\")\n",
    "\n",
    "\n",
    "def parse_year(s: str):\n",
    "    \"\"\"maker data の date of birth / death を parse する method\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(s, float):\n",
    "        return s\n",
    "\n",
    "    if '-' not in s:\n",
    "        return int(s)\n",
    "\n",
    "    return int(s.split('-')[0])\n",
    "\n",
    "\n",
    "class MakerAgeBlock(BaseBlock):\n",
    "    '''principal_makerの生まれた年/亡くなった年や制作開始/終了時点での年齢\n",
    "    '''\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        maker_df = art_ds.maker_df\n",
    "        output_df = maker_df[[\"name\"]].copy()\n",
    "\n",
    "        output_df[\"birth_year\"] = maker_df[\"date_of_birth\"].map(parse_year)\n",
    "        output_df[\"death_year\"] = maker_df[\"date_of_death\"].map(parse_year)\n",
    "        output_df[\"living_year\"] = output_df[\"death_year\"] - output_df[\"birth_year\"]\n",
    "        self.agg_df = output_df\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = merge_by_key(input_df[\"principal_maker\"].rename(\"name\"), self.agg_df, on=\"name\")\n",
    "        output_df[\"age_in_dating_early\"] = input_df[\"dating_year_early\"] - output_df[\"birth_year\"]\n",
    "        output_df[\"age_in_dating_late\"] = input_df[\"dating_year_late\"] - output_df[\"birth_year\"]\n",
    "\n",
    "        return output_df.add_prefix(\"MakerAge_\")\n",
    "\n",
    "\n",
    "class ObjectYearMetaBlock(BaseBlock):\n",
    "    '''objectの制作開始/終了/期間などに関する特徴\n",
    "    '''\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = pd.DataFrame()\n",
    "\n",
    "        output_df[\"acquisition_year\"] = pd.to_datetime(input_df[\"acquisition_date\"]).dt.year\n",
    "        output_df[\"dating_sorting_date\"] = input_df[\"dating_sorting_date\"]\n",
    "        output_df[\"dating_period\"] = input_df[\"dating_period\"]\n",
    "        output_df[\"dating_year_early\"] = input_df[\"dating_year_early\"]\n",
    "        output_df[\"dating_year_late\"] = input_df[\"dating_year_late\"]\n",
    "        output_df[\"diff_dating_year\"] = output_df[\"dating_year_late\"] - output_df[\"dating_year_early\"]\n",
    "        output_df[\"diff_acquisition_dating\"] = output_df[\"acquisition_year\"] - output_df[\"dating_year_late\"]\n",
    "\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class PrincipalMakerCountByObjectBlock(BaseBlock):\n",
    "    '''object_id単位でのprincipal maker/principal occupationのcrosstab集計\n",
    "    '''\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        maker_df = art_ds.principal_maker_df\n",
    "        occupation_df = art_ds.principal_occupation_df\n",
    "        agg_df = pd.DataFrame()\n",
    "\n",
    "        for col in [\"qualification\", \"roles\", \"productionPlaces\"]:\n",
    "            _df = pd.crosstab(maker_df[\"object_id\"], maker_df[col])\n",
    "\n",
    "            agg_df = pd.concat([_df.add_prefix(f\"{col}_\"), agg_df], axis=1)\n",
    "\n",
    "        occupation_maker_df = occupation_df.merge(maker_df[[\"id\", \"object_id\"]], on=\"id\", how=\"left\")\n",
    "        occupation_count_df = pd.crosstab(occupation_maker_df[\"object_id\"], occupation_maker_df[\"name\"]).add_prefix(\"occupation_\")\n",
    "        agg_df = agg_df.merge(occupation_count_df, on=\"object_id\", how=\"left\")\n",
    "\n",
    "        self.agg_df = agg_df\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = merge_by_key(input_df, self.agg_df).fillna(0).astype(int)\n",
    "\n",
    "        return output_df.add_prefix(\"PrincipalMakerByObject_\")\n",
    "\n",
    "\n",
    "class PrincipalMakerCountByMakerBlock(BaseBlock):\n",
    "    '''principal_maker単位でのprincipal maker/principal occupationのcrosstab集計\n",
    "    '''\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        maker_df = art_ds.principal_maker_df\n",
    "        occupation_df = art_ds.principal_occupation_df\n",
    "        agg_df = pd.DataFrame()\n",
    "\n",
    "        for col in [\"qualification\", \"roles\", \"productionPlaces\"]:\n",
    "            _df = pd.crosstab(maker_df[\"maker_name\"], maker_df[col])\n",
    "\n",
    "            agg_df = pd.concat([_df.add_prefix(f\"{col}_\"), agg_df], axis=1)\n",
    "\n",
    "        occupation_maker_df = occupation_df.merge(maker_df[[\"id\", \"maker_name\"]], on=\"id\", how=\"left\")\n",
    "        occupation_count_df = pd.crosstab(occupation_maker_df[\"maker_name\"], occupation_maker_df[\"name\"]).add_prefix(\"occupation_\")\n",
    "        agg_df = agg_df.merge(occupation_count_df, on=\"maker_name\", how=\"left\")\n",
    "\n",
    "        agg_df.index = agg_df.index.rename(\"principal_maker\")\n",
    "        self.agg_df = agg_df\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = merge_by_key(input_df, self.agg_df, on=\"principal_maker\").fillna(0).astype(int)\n",
    "\n",
    "        return output_df.add_prefix(\"PrincipalMakerByMaker_\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringLengthBlock(BaseBlock):\n",
    "    '''文字列の長さを返す\n",
    "    '''\n",
    "    def __init__(self, columns: list):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = pd.DataFrame()\n",
    "        for col in self.columns:\n",
    "            output_df[col] = input_df[col].fillna(\"\").str.len()\n",
    "\n",
    "        return output_df.add_prefix(\"StringLength_\")\n",
    "\n",
    "\n",
    "class WordCountBlock(BaseBlock):\n",
    "    '''train/testの単語数をカウント\n",
    "    '''\n",
    "    def __init__(self, columns: list):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        output_df = pd.DataFrame()\n",
    "        for col in self.columns:\n",
    "            output_df[col] = input_df[col].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "\n",
    "        return output_df.add_prefix(\"WordCount_\")\n",
    "\n",
    "    \n",
    "def text_normalization(text):\n",
    "    '''textheroを用いたテキスト前処理pipeline\n",
    "    '''\n",
    "    # nltkのオランダ語と英語のstopword\n",
    "    custom_stopwords = nltk.corpus.stopwords.words('dutch') + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    clean_text = hero.clean(\n",
    "        text, \n",
    "        pipeline=[\n",
    "            hero.preprocessing.fillna,\n",
    "            hero.preprocessing.lowercase,\n",
    "            hero.preprocessing.remove_digits,\n",
    "            hero.preprocessing.remove_punctuation,\n",
    "            hero.preprocessing.remove_diacritics,\n",
    "            lambda x: hero.preprocessing.remove_stopwords(x, stopwords=custom_stopwords),\n",
    "            hero.preprocessing.remove_whitespace,\n",
    "        ])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "class TfidfBlock(BaseBlock):\n",
    "    '''TF-IDF特徴量\n",
    "    - 複数カラムに対応\n",
    "    '''\n",
    "    def __init__(self, column: str, n_components: int = 50):\n",
    "        self.column = column\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def get_text_series(self, input_df: pd.DataFrame) -> pd.Series:\n",
    "        '''input_dfを入力としてテキスト正規化したpd.Seriesを返す\n",
    "        &で連結された複数カラムの場合は空白区切りでテキストを連結させる\n",
    "        '''\n",
    "        out_series = None\n",
    "\n",
    "        for i, col in enumerate(self.column.split('&')):\n",
    "            text_i = text_normalization(input_df[col]).astype(str)\n",
    "            if out_series is None:\n",
    "                out_series = text_i\n",
    "            else:\n",
    "                out_series = out_series + ' ' + text_i\n",
    "\n",
    "        return out_series\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        whole_df = art_ds.get_whole_df()\n",
    "        x = self.get_text_series(whole_df)\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=20000)),\n",
    "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=42)),\n",
    "        ])\n",
    "\n",
    "        feature = self.pipeline.fit_transform(x)\n",
    "        self.agg_df = pd.concat([whole_df[[OBJECT_ID]].copy(), pd.DataFrame(feature)], axis=1)\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return merge_by_key(input_df, self.agg_df).add_prefix(f\"{self.column}_Tfidf_\")\n",
    "\n",
    "\n",
    "def get_w2v_input_df():\n",
    "    '''w2vのinputのdataframeを作成する\n",
    "    - object_idをPKとするdataframeを返すことを想定\n",
    "    '''\n",
    "    # whole_df(train/test)\n",
    "    whole_df = art_ds.get_whole_df()\n",
    "    whole_df[\"title\"] = whole_df[\"title\"].astype(str)\n",
    "    whole_df[\"description\"] = whole_df[\"description\"].astype(str)\n",
    "    whole_df[\"more_title\"] = whole_df[\"more_title\"].astype(str)\n",
    "    whole_df[\"title-description\"] = whole_df[\"title\"].astype(str) + ' ' + whole_df[\"description\"].astype(str)\n",
    "\n",
    "    w2v_whole_title = whole_df.groupby([\"object_id\"])[\"title\"].apply(list).reset_index()\n",
    "    w2v_whole_description = whole_df.groupby([\"object_id\"])[\"description\"].apply(list).reset_index()\n",
    "    w2v_whole_more_title = whole_df.groupby([\"object_id\"])[\"more_title\"].apply(list).reset_index()\n",
    "    w2v_whole_title_description = whole_df.groupby([\"object_id\"])[\"title-description\"].apply(list).reset_index()\n",
    "    \n",
    "    # material\n",
    "    material_df = art_ds.material_df.copy()\n",
    "    w2v_material = material_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # object\n",
    "    object_df = art_ds.object_df.copy()\n",
    "    w2v_object = object_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # technique\n",
    "    technique_df = art_ds.technique_df.copy()\n",
    "    w2v_technique = technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/object\n",
    "    material_object_df = pd.concat([material_df, object_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_object = material_object_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/technique\n",
    "    material_technique_df = pd.concat([material_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_technique = material_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # object/technique\n",
    "    object_technique_df = pd.concat([object_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_object_technique = object_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material/object/technique\n",
    "    material_object_technique_df = pd.concat([material_df, object_df, technique_df], axis=0).reset_index(drop=True)\n",
    "    w2v_material_object_technique = material_object_technique_df.groupby([\"object_id\"])[\"name\"].apply(list).reset_index()\n",
    "\n",
    "    # material + title + description + more_title\n",
    "\n",
    "\n",
    "    returns = {\n",
    "        \"w2v_whole_title\": w2v_whole_title,\n",
    "        \"w2v_whole_description\": w2v_whole_description,\n",
    "        \"w2v_whole_more_title\": w2v_whole_more_title,\n",
    "        \"w2v_whole_title_description\": w2v_whole_title_description,\n",
    "        \"w2v_material\": w2v_material,\n",
    "        \"w2v_object\": w2v_object,\n",
    "        \"w2v_technique\": w2v_technique,\n",
    "        \"w2v_material_object\": w2v_material_object,\n",
    "        \"w2v_material_technique\": w2v_material_technique,\n",
    "        \"w2v_object_technique\": w2v_object_technique,\n",
    "        \"w2v_material_object_technique\": w2v_material_object_technique,\n",
    "    }\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "class W2vBlock(BaseBlock):\n",
    "    '''Word2Vecによる単語ベクトル表現を得て、平均により文章ベクトル化\n",
    "    - 単一dataframeで複数カラムに対応\n",
    "    '''\n",
    "    def __init__(self, sentences_df: pd.DataFrame, df_name: str, vector_size: int, min_count: int, window: int, epochs: int):\n",
    "        self.sentences_df = sentences_df\n",
    "        self.df_name = df_name\n",
    "        self.vector_size = vector_size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        self.agg_df = art_ds.get_whole_df()[['object_id']]\n",
    "        cat_col = self.sentences_df.columns.drop(\"object_id\")[0]\n",
    "        # text normalization\n",
    "        self.sentences_df[cat_col] = text_normalization(self.sentences_df[cat_col])\n",
    "\n",
    "        self.w2v_model = word2vec.Word2Vec(\n",
    "            self.sentences_df[cat_col].values.tolist(),\n",
    "            vector_size=self.vector_size,\n",
    "            min_count=self.min_count,\n",
    "            window=self.window,\n",
    "            epochs=self.epochs,\n",
    "            sg=1,\n",
    "        )\n",
    "\n",
    "        # element-wise average(SWEM-aver)\n",
    "        sentence_vectors = self.sentences_df[cat_col].progress_apply(lambda x: np.mean([self.w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack(sentence_vectors)\n",
    "        sentence_vectors_df = pd.DataFrame(sentence_vectors, columns=[f\"{self.df_name}_{i}\" for i in range(self.vector_size)])\n",
    "        sentence_vectors_df.index = self.sentences_df[\"object_id\"]\n",
    "        self.agg_df = self.agg_df.merge(sentence_vectors_df, on='object_id', how='left')\n",
    "        \n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame):\n",
    "        return merge_by_key(input_df, self.agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## palette feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(train_df: pd.DataFrame, test_df: pd.DataFrame, y, blocks: list) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_feat_df = pd.DataFrame()\n",
    "    test_feat_df = pd.DataFrame()\n",
    "\n",
    "    for block in blocks:\n",
    "        with utils.timer(name=f\"{str(block) + '_fit'}\", logger=logger):\n",
    "            try:\n",
    "                out_train_block = block.fit(train_df, y=y)\n",
    "            except Exception as e:\n",
    "                print(f'Error on {block} fit. ')\n",
    "                raise e from e\n",
    "\n",
    "            assert len(out_train_block) == len(train_df), block\n",
    "\n",
    "        train_feat_df = pd.concat([train_feat_df, out_train_block], axis=1)\n",
    "\n",
    "    for block in blocks:\n",
    "        with utils.timer(name=f\"{str(block) + '_transform'}\", logger=logger):\n",
    "            out_test_block = block.transform(test_df)\n",
    "            assert len(out_test_block) == len(test_df), block\n",
    "\n",
    "        test_feat_df = pd.concat([test_feat_df, out_test_block], axis=1)\n",
    "\n",
    "    return train_feat_df, test_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_k_fold(train_df: pd.DataFrame, group_col: str, n_splits: int = 5, seed: int = 42) -> List[tuple]:\n",
    "    '''GroupKFoldで分割してfold列を付与する\n",
    "    '''\n",
    "    _train_df = train_df.copy()\n",
    "    group_series = _train_df[group_col]\n",
    "    group_key = group_series.unique()\n",
    "\n",
    "    splitter = KFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    fold = []\n",
    "\n",
    "    for fold_id, (tr_group_idx, val_group_idx) in enumerate(splitter.split(group_key)):\n",
    "        tr_group = group_key[tr_group_idx]\n",
    "        val_group = group_key[val_group_idx]\n",
    "\n",
    "        is_tr = group_series.isin(tr_group)\n",
    "        is_val = group_series.isin(val_group)\n",
    "\n",
    "        train_idx = np.array(_train_df[is_tr].index)\n",
    "        valid_idx = np.array(_train_df[is_val].index)\n",
    "\n",
    "        fold.append((train_idx, valid_idx))\n",
    "\n",
    "    return fold\n",
    "\n",
    "\n",
    "def get_stratified_k_fold(train_df: pd.DataFrame, y: np.ndarray, n_splits: int = 5, seed: int = 42) -> List[tuple]:\n",
    "    '''StratifiedK-Foldで分割してfold列を付与\n",
    "    '''\n",
    "    X_train = train_df.copy()\n",
    "    y_train = y.round()\n",
    "\n",
    "    splitter = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    fold = []\n",
    "\n",
    "    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(X_train, y_train)):\n",
    "        fold.append((train_idx, valid_idx))\n",
    "\n",
    "    return fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        params: dict,\n",
    "        cv: List[tuple],\n",
    "        seeds: list,\n",
    "    ):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.params = params\n",
    "        self.cv = cv\n",
    "        self.seeds = seeds\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self):\n",
    "        oof = np.zeros((len(self.seeds), len(self.y_train)))\n",
    "\n",
    "        for i, seed in enumerate(self.seeds):\n",
    "            oof_ = np.zeros((len(self.y_train)))\n",
    "            self.params[\"seed\"] = seed\n",
    "\n",
    "            for train_idx, valid_idx in self.cv:\n",
    "                X_train_fold = self.X_train.iloc[train_idx].values\n",
    "                X_valid_fold = self.X_train.iloc[valid_idx].values\n",
    "\n",
    "                y_train_fold = self.y_train[train_idx]\n",
    "                y_valid_fold = self.y_train[valid_idx]\n",
    "\n",
    "                train_set = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "                valid_set = lgb.Dataset(X_valid_fold, y_valid_fold, reference=train_set)\n",
    "\n",
    "                model = lgb.train(\n",
    "                    train_set=train_set,\n",
    "                    valid_sets=[train_set, valid_set],\n",
    "                    params=self.params,\n",
    "                    verbose_eval=100,\n",
    "                    # callbacks=[wandb_callback()]\n",
    "                )\n",
    "\n",
    "                oof_[valid_idx] = model.predict(\n",
    "                    X_valid_fold,\n",
    "                    num_iteration=model.best_iteration,\n",
    "                )\n",
    "                \n",
    "                self.models.append(model)\n",
    "\n",
    "            oof[i, :] = oof_\n",
    "        \n",
    "        y_oof = np.mean(oof, axis=0)\n",
    "\n",
    "        return y_oof, self.models\n",
    "\n",
    "    def predict(self):\n",
    "        y_pred = np.mean(\n",
    "            [model.predict(self.X_test) for model in self.models], axis=0\n",
    "        )\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 5,\n",
    "    \"num_leaves\": 32,\n",
    "    \"lambda_l1\": 0.01,\n",
    "    \"lambda_l2\": 0.01,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"num_threads\": 8,\n",
    "    \"verbosity\": -1,\n",
    "    \"num_iterations\": 10000,\n",
    "    \"early_stopping_round\": 100,\n",
    "}\n",
    "# wandb.config.update(lgb_params)\n",
    "seeds = [42, 2021, 2434, 1123, 98]\n",
    "\n",
    "# feature engineering\n",
    "train_df = art_ds.train_df\n",
    "test_df = art_ds.test_df\n",
    "y_train = art_ds.get_target(log=True)\n",
    "cv = get_stratified_k_fold(train_df, y_train, 5, 42)\n",
    "\n",
    "blocks = [\n",
    "    # table/aggregate features\n",
    "    OrdinalEncodingBlock(\n",
    "        cat_cols=[\n",
    "            \"principal_maker\",\n",
    "            \"principal_or_first_maker\",\n",
    "            \"copyright_holder\",\n",
    "            \"acquisition_method\",\n",
    "        ]\n",
    "    ),\n",
    "    CountEncodingBlock(\n",
    "        cat_cols=[\n",
    "            \"principal_maker\",\n",
    "            \"principal_or_first_maker\",\n",
    "            \"acquisition_method\",\n",
    "        ]\n",
    "    ),\n",
    "    TargetEncodingBlock(\n",
    "        columns=[\"principal_maker\"],\n",
    "        target_column=\"likes\",\n",
    "        cv=cv,\n",
    "    ),\n",
    "    *[OtherTableCountBlock(\n",
    "    category_df=df,\n",
    "    df_name=name,\n",
    "    minimum_freq=min_freq,\n",
    "    )\n",
    "    for df, name, min_freq in zip(\n",
    "        [art_ds.material_df, art_ds.technique_df, art_ds.production_df, art_ds.historical_df, art_ds.object_df],\n",
    "        [\"material\", \"technique\", \"production_place\", \"historical_person\", \"object_collection\"],\n",
    "        [30, 30, 30, 30, 30])\n",
    "    ],\n",
    "    SubtitleSizeBlock(),\n",
    "    MakerAgeBlock(),\n",
    "    ObjectYearMetaBlock(),\n",
    "    PrincipalMakerCountByObjectBlock(),\n",
    "    PrincipalMakerCountByMakerBlock(),\n",
    "    # text feature\n",
    "    StringLengthBlock(\n",
    "        columns=[\n",
    "            'title',\n",
    "            'description',\n",
    "            'long_title',\n",
    "            'more_title',\n",
    "            'sub_title',\n",
    "        ]\n",
    "    ),\n",
    "    WordCountBlock(\n",
    "        columns=[\n",
    "            'title',\n",
    "            'description',\n",
    "            'long_title',\n",
    "            'more_title',\n",
    "            'sub_title',\n",
    "        ]\n",
    "    ),\n",
    "    *[TfidfBlock(\n",
    "        column=column,\n",
    "        n_components=50) for column in [\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"long_title\",\n",
    "            \"more_title\",\n",
    "            \"title&description&long_title&more_title\",\n",
    "            \"acquisition_credit_line\",\n",
    "        ]\n",
    "    ],\n",
    "    *[W2vBlock(\n",
    "        sentences_df=sentences_df,\n",
    "        df_name=df_name,\n",
    "        vector_size=50,\n",
    "        min_count=1,\n",
    "        window=5,\n",
    "        epochs=100\n",
    "        ) for df_name, sentences_df in get_w2v_input_df().items()\n",
    "    ],\n",
    "]\n",
    "\n",
    "X_train, X_test = create_feature(train_df, test_df, y_train, blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/inference\n",
    "trainer = LightGBMTrainer(X_train, y_train, X_test, lgb_params, cv, seeds)\n",
    "y_oof, models = trainer.fit()\n",
    "y_pred = trainer.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importance(models, feat_train_df) -> plotly.graph_objects.Figure:\n",
    "    '''LightGBMのfeature importanceを可視化\n",
    "    '''\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importance(importance_type=\"gain\")\n",
    "        _df['feature'] = feat_train_df.columns\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], \n",
    "                                          axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby('feature')\\\n",
    "        .sum()[['feature_importance']]\\\n",
    "        .sort_values('feature_importance', ascending=False).index[:50]\n",
    "    \n",
    "    fig = px.box(\n",
    "        feature_importance_df.query(\"feature in @order\"),\n",
    "        x=\"feature_importance\",\n",
    "        y=\"feature\",\n",
    "        category_orders={\"feature\": order},\n",
    "        width=1250,\n",
    "        height=900,\n",
    "        title=\"Top 50 feature importance\",\n",
    "    )\n",
    "    fig.update_yaxes(showgrid=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_oof_predict_distribution(y_train: np.ndarray, y_oof: np.ndarray, y_pred: np.ndarray) -> plotly.graph_objects.Figure:\n",
    "    '''train, oof, predのhistogramを可視化する\n",
    "    '''\n",
    "    dfs = [pd.DataFrame({'phase': phase, 'value': value}) for phase, value in zip(['train', 'oof', 'pred'], [y_train, y_oof, y_pred])]\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    fig = px.histogram(df, x=\"value\", color=\"phase\", width=1250, marginal=\"box\")\n",
    "    fig.update_layout(barmode=\"overlay\")\n",
    "    fig.update_traces(opacity=0.6, marker=dict(line=dict(width=2,color=\"DarkSlateGrey\")))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_feature_importance(models, X_train)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_oof_predict_distribution(y_train, y_oof, y_pred)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_log_target(y_log: np.ndarray) -> np.ndarray:\n",
    "    y = np.expm1(y_log)\n",
    "    y = np.where(y < 0, 0, y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_submit(submission: pd.DataFrame, y_pred: np.ndarray, path: str, f_name: str):\n",
    "    dir_path = Path(path)\n",
    "    submission_df = submission.copy()\n",
    "    submission_df[\"likes\"] = y_pred\n",
    "    assert len(submission_df) == len(y_pred)\n",
    "\n",
    "    submission_df.to_csv(dir_path / str(f_name + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_pred = revert_log_target(y_pred)\n",
    "make_submit(art_ds.submission, revert_pred, OUTPUT_DIR, EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python389jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "4f969c46b9e489a3ba44234dd60c0d7afd42f210e566a36195393afc0eaa93a9"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}